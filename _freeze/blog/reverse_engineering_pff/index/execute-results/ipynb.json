{
  "hash": "e08006b6d7ccdff19ba29a54205eeaec",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Reverse-engineering a private API to build a high-quality Python package\"\ndescription: \"PFF, the gold standard for NFL analytics, does not provide easy access to extract their data. This article describes how I found their internal APIs and turned it into a strongly-typed and opionionated Python package\"\nauthor: \"Abhi Agarwal\"\ndate: \"2023-04-30\"\ncategories: [nfl, python]\ndraft: true\n---\n\n\n\n\n\n\n# Introduction\n\nThe world of sports data and analytics is becoming an increasingly prominent field. Sports are fundamentally a zero-sum game, meaning every edge you have can be the difference between carrying a trophy and going home. A popular avenue for sports data for America's most common popular is Pro-Football-Focus (PFF), who employ analysts who watch every single player on every single play by watching film to assign a numerical grade measuring a player's performance. Football is a sport where 22 people on the field influence a play, but often only two-to-four of those players have their action recorded in the official game log. PFF aims to solve this problem. PFF's imputed statistics have shown to be as stable or more than advanced play-by-play based data such as EPA/play and DVOA.\n\nPFF sells a subscription providing access to their database. However, this is only accessible through browser, and it fails to provide export functionality except for a select few datasets. When doing longitudinal data analysis, this can be frustrating and limiting. I wrote my economics thesis primarily using PFF data, and I can tell you that scraping the data took me weeks of time that could have been spent writing and conducting research. There has to be a better way.\n\nAnd there is! By using some rudimentary reverse-engineering techniques, I was able to create a fully functioning python library that provides thoughtful abstractions to underlying PFF data, with *zero scraping*. This article serves as a documentation of how I was able to solve this problem, the design decisions I took, and wall as an informal guide for how you can also try and solve these kind of problems in the wild.\n\n# The naive approach with `BeautifulSoup`\n\nThe first instinct when attempting to automate the collection of any sort of online content is to use the python library [`BeautifulSoup`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) to parse HTML. Let's take a look at an example PFF page[^1].\n\n[^1]: Since PFF's grading is properitary data, I wrote a quick and dirty javascript script that randomizes all the grades to be 1-100.\n\n![https://premium.pff.com/nfl/teams/2022/REGPO](assets/images/pff_screenshot_team_overview.png)\n\nThis is tabular, so well suited for `BeautifulSoup`. Let's try and write an implementation to parse this out.\n\n::: {#90e429bb .cell execution_count=1}\n``` {.python .cell-code}\ndef hello_world() -> str:\n    return \"Hello World\"\n\nhello_world()\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\n'Hello World'\n```\n:::\n:::\n\n\n# Reverse-engineering the API\n\n# Creating a Python package\n\nDependency management in python is hell. Much like the classic xckd, there's a million different tools and all the attempts to unify said tools have only created the n+1 tool. I prefer using `poetry` --- it reminds me most of `cargo`, although it currently isn't compliant with [PEP-621](https://peps.python.org/pep-0621/).\n\nThere are a few developer tools I include with every single project: `black` for formatting, `ruff` for linting, `mypy` for static type-checking[^2]. Let's add those too:\n\n[^2]: For 99% of projects, I recommend `pyright` over `mypy`. I'm only using the latter because `pydantic` has a plugin that allows `mypy` to understand its sematics.\n\n``` {.zsh filename=\"Terminal\"}\npoetry add --group dev black ruff mypy\n```\n\nAnd in our `pyproject.toml`, let's set up some defaults.\n\n``` {.toml filename=\"pyproject.toml\"}\n[tool.ruff]\nline-length = 120\ntarget-version = \"py38\"\nselect = [\"ALL\"]\nignore = [\n    \"TCH\", # pydantic run-time type checking is needed\n    \"D100\" # I don't want to\n]\nsrc = [\"pypff\", \"test\"]\n\n[tool.ruff.isort]\nknown-first-party = [\"pypff\"]\nrequired-imports = [\"from __future__ import annotations\"]\n\n[tool.ruff.pydocstyle]\nconvention = \"google\"\n\n[tool.black]\nline-length = 120\n\n[tool.mypy]\nplugins = [\"pydantic.mypy\"]\nstrict = true\n```\n\n::: {.callout-note collapse=\"true\"}\n### Why I don't use `pre-commit`\n`pre-commit` is a popular tool for ensuring that tests, linting, formatting, etc. is done before as a git hook before comitting your code. I'm not a big fan of its design. I heavily dislike the way it runs tools, essentially pulling the tool from the internet and running it in an isolated environment. It seems like unncessary duplication given my dev tools are already installed in my venv, and also requires you to take care that the version number in the `pre-commit` config matches the declared version in `pyproject.toml`, which gets tedious fast. I prefer using CI as my source of truth.\n:::\n\nWhile this stuff may seem trivial, setting up a good tooling environment is vital. Python is a deeply complicated beast, and not a single human alive has the capacity to write good, maintainable code without someone holding your hand and slapping you if you're wrong. By declaring your dev tools in your manifest, it creates a reproducible environment so any person in the world can immediately start contributing to your project. We've given the responsibilities to the impartial computer, no one has to argue anymore about stylistic checks.\n\n## Mirroring the API through directory structure\n\nI'd like to preface this by saying this is the dumbest and most brilliant thing I've ever done.\n\n## Using `pydantic` to control data structure\n\n`Pydantic` is one of my favorite Python libraries, and broadly, represents everything I love about Python. It provides a safe and robust way to take data, validate it, and transform it into the desired output. The API I'm working with is *not* be trusted --- it could change at any moment with zero notice, and Python's flexibility could shoot us in the foot by letting us propogate faulty data to library users. Pydantic provides a mechanism to ensure that we prioritize **correctness** for our library consumers.\n\nFor example of where `pydantic` can increase the quality of our library, let's take a look at an example JSON payload.\n\n``` {.json include=\"assets/code/leagues_payload.json\"}\n```\n\nOur data is a heavily nested structure, meaning that if a library consumer wants to manipulate this data, they also need to know *how* the data is structured. This is a herculean task for anyone, and drastically decreases the time spent actually conducting analysis. In addition, since our payload is intended for the PFF website, they include properties like `default_week` or `display_order` for the frontend that is completely irrelevant to us. We want to hide those details from our library users.\n\nHere's how I'd model this JSON through Pydantic models.\n\n::: {#d1ecc584 .cell execution_count=2}\n``` {.python .cell-code}\nfrom pydantic import BaseModel, Field\n\nclass WeekGroup(BaseModel):\n    default: bool = Field(exclude=True)\n    group_name: str = Field(alias=\"label\")\n    group_abbreviation: str = Field(alias=\"value\")\n    weeks: list[int]\n\nclass Week(BaseModel):\n    week_abbreviation: str = Field(alias=\"abbreviation\")\n    all_star: bool\n    display_order: int = Field(exclude=True)\n    week_number: int = Field(alias=\"id\")\n    week_name: str = Field(alias=\"name\")\n    post_season: bool\n    pre_season: bool\n    regular_season: bool\n\nclass League(BaseModel):\n    league_abbreviation: str = Field(alias=\"abbreviation\")\n    default_season: int = Field(exclude=True)\n    default_week: int = Field(exclude=True)\n    default_week_group: str = Field(exclude=True)\n    league_id: int = Field(alias=\"id\")\n    league_name: str = Field(alias=\"name\")\n    seasons: list[int]\n    slug: str = Field(exclude=True)\n    week_groups: list[WeekGroup]\n    weeks: list[Week]\n```\n:::\n\n\nThis breaks down our complex JSON into an easily understandable block of Python. I use the `pydantic.dataclasses` feature as a drop-in replacement for the standard python `dataclass`, so people using the library are more familiar with its semantics. By using the `Field` property, we're able to exclude certain elements in the model output while still ensuring they exist for correctness purposes. We're able to turn a plain integer into a strongly-typed integer `WeekNum` that preserves its meaning and allows for introducing additional validation if desired. And perhaps most importantly, we've now broken up our JSON payload into composable parts --- let's say another endpoint uses `WeekGroup` in its output (spoiler: it does), we can just import this `WeekGroup` and get the same exact semantics for free.\n\nIf you couldn't tell, I freaking *love* pydantic.\n\nAs a test, let's take our JSON payload and convert it into a Pydantic model.\n\n::: {#a13b1e0d .cell execution_count=3}\n``` {.python .cell-code}\n# from pathlib import Path\n# from rich.pretty import pprint\n# import json\n# from IPython.display import display\n\n# json_payload = json.loads((Path(\".\") / \"assets\" / \"code\" / \"leagues_payload.json\").read_text())\n\n# from pydantic.analyzed_type import AnalyzedType\n\n# parsed_model = AnalyzedType(list[League]).validate_python(json_payload[\"leagues\"])\n# pprint(parsed_model[0].model_dump())\n```\n:::\n\n\n## (Ab)using polymorphism\n\nMy first language was C++, so I've been proudly stockholm-syndromed into loving object-oriented paradigms, especially polymorphism. And although a simple API client doesn't seem to tend itself well to composition, by exploiting common data structures, we can turn our library into a composition of composed models being composited with composed models.\n\n## To Async, or to Sync[^3]\n\n[^3]: *Try saying this 10 times fast*\n\nI want to talk about the topic I dread the most about python: the `async` problem. We are taught to reason about code in a synchronous way, but modern computers have multiple cores and thus the ability to multitask --- asynchronous execution runs code in a way that our computers can optimize. Python has a relatively new mechanism to provide different semantics for these tasks --- `async` --- but it comes with its own pitfalls.\n\nAs a library author, there are three approaches I can take here:\n\n### Async-only library\n\nWith the async version, we provide the \"correct\" API, but Async code *infects* other code as when you call an async call, all your code up the stack **must** be async. This is known as the [function coloring problem](https://journal.stuffwithstuff.com/2015/02/01/what-color-is-your-function/). This wouldn't be a huge problem if the Python community had async habits, but unfortunately the functionality was tacked onto python years into its lifecycle, so most code assumes a sync context^[Just from personal experience in the sports data science community, I have yet to see a single use of async in the wild.]. I want people to use this library, I'd be losing active consumers who choose not to bother rewriting all their code. In addition, I want this package to be accessible to R users, and\n`reticulate` currently [does not support async execution](https://github.com/rstudio/reticulate/issues/515).\n\nAs much as I dream of a world where pure async is feasible, we're not there yet.\n\n### Sync-only library\n\nProviding a purely synchronous library is the easiest solution, but we're leaving performance on the table. Our library is just a HTTP client, which provides the majority of the time spent, with a few levels of abstraction on top. This may not seem like a big problem, up until you're running multiple requests in a loop to pull data from multiple years, in which case your execution time can exponentially increases.\n\n### Porque no los dos?\n\nAlright, but what if we tried to mix both variants and provide both an asynchronous and synchronous API! Unfortunately, it's not easy as it seems. We'd have to maintain two different sets of APIs --- that increases maintiance costs and violates DRY, as well as increasing the likelihood that the two variants go out of sync^[heh] if we change any code. We can try to maintain one API and provide a wrapper that translates a call into its partner, but that requires spawning a new event loop that likely has a run-time cost that we're trying to avoid in the first place.\n\n### My solution\n\nOh yes, you guessed it, I'm gonna solve this problem with *more* polymorphism. And the solution is relatively easy â€” on our `Endpoint` class, let's just define a `get_async()` method that does the exact same thing as `get()` except, well, async. Because of the way we structured our original class, we can just copy-paste `get()`, use `httpx.AsyncClient()` instead, sprinkle in an `await` whenever VSCode complains, and we've added async functionality completely for free on every single one of our endpoints with about 5 lines of code.\n\n## Building an opinionated library\n\nUsing this, we now have a decidingly unopinionated and *boring* API. If we know the location of an endpoint and the parameters we want, we can pull that exact data pretty easily. But most people don't have the time to dig through documentation to read every single parameter, what they want is a way for them to be able to quickly iterate\n\n## Writing documentation with `sphinx`\n\nI come from a non-traditional programming background. I graduated with a degree in Mathematics and Economics, and didn't have any formal or educational programming experience outside of messing with a few hobbyist Python scripts[^6]. I picked up my programming skills from reading technical blogs. I once heard an adage, that's probably popular in programming circles, that stuck with me deeply about the nature of code.\n\n[^6]: and writing Stata, which I'd rather not talk about.\n\n[**It's harder to read code than to write it**](https://www.joelonsoftware.com/2000/04/06/things-you-should-never-do-part-i/)\n\nI was able to read some old shitty C++ code I wrote eight years ago and knew exactly what it meant. I can ensure you, dear reader, that you would not be able to. We write code so other people can figure out what the fuck we're doing.\n\n## Testing, CI, Building, and Publishing\n\n# We do a little bit of Analysis\n\nWe have our library! `pypff` is live.\n\nI prefer using `ggplot2` as my graphing library, so I'll be conducting the majority of my analysis in R using the `tidyverse` ecosystem. Don't worry if you don't understand R, the tidyverse ecosystem is designed around having composable syntax.\n\n# A note on ethics\n\nThe ethics of API scraping is a difficult topic. This package enables access to the exact same data exposed on the PFF website, albeit in a more programatic way. Is there anything illegal about it? Absolutely not. Is it against PFF's Terms of Service? I'm not sure, but as it's currently written, I'm leaning on \"no\". However, good etiquette is paramount, don't spam the website with requests and use your `sleep()` functions liberally.\n\n# Conclusion\n\nWhen writing a python library, we need to prioritize two things --- **abstraction** and **correctness**. We want to ensure that our consumers are able to reason about the API in a way that fits their intutition, and also want to provide them with a promise that what they're expecting to see is exactly what they're going to get.\n\n---\njupyter:\n  kernelspec:\n    display_name: Python 3 (ipykernel)\n    language: python\n    name: python3\n    path: /Users/abhiagarwal/Developer/abhiaagarwal/.venv/share/jupyter/kernels/python3\n  language_info:\n    codemirror_mode:\n      name: ipython\n      version: 3\n    file_extension: .py\n    mimetype: text/x-python\n    name: python\n    nbconvert_exporter: python\n    pygments_lexer: ipython3\n    version: 3.12.8\n---\n",
    "supporting": [
      "index_files/figure-ipynb"
    ],
    "filters": []
  }
}